{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning a Model on T5x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page outlines the steps to fine-tune an existing pre-trained model with T5X on common downstream tasks defined with SeqIO. This is one of the simplest and most common use cases of T5X. If you're new to T5X, this tutorial is the recommended starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a model with T5X consists of the following steps:\n",
    "\n",
    "1. Choose the pre-trained model to fine-tune.\n",
    "2. Choose the SeqIO Task/Mixture to fine-tune the model on.\n",
    "3. Write a Gin file that configures the pre-trained model, SeqIO Task/Mixture and other details of your fine-tuning run.\n",
    "4. Configure a Vertex AI Training job to fine tune the model.\n",
    "5. Monitor your job and parse metrics.\n",
    "\n",
    "These steps are explained in detail in the following sections. An example run that fine-tunes a T5-small checkpoint on WMT14 English to German translation benchmark is also showcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Choose a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pre-trained model, you need a Gin config file that defines the model params, and the model checkpoint to load from. For your convenience, TensorFlow checkpoints and Gin configs for common T5 pre-trained models have been made available for use in T5X. A list of all the available pre-trained models (with model checkpoints and Gin config files) are available in the Models documentation.\n",
    "\n",
    "For the example run, you will use the T5 1.1 Small model. The Gin file for this model is located at `/t5x/examples/t5/t5_1_1/small.gin`, and the checkpoint is located at `gs://t5-data/pretrained_models/t5x/t5_1_1_small`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose a SeqIO Task/Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SeqIO Task encapsulates the data source, the preprocessing logic to be performed on the data before querying the model, the postprocessing logic to be performed on model outputs, and the metrics to be computed given the postprocessed outputs and targets. A SeqIO Mixture denotes a collection of Tasks and enables fine-tuning a model on multiple Tasks simultaneously.\n",
    "\n",
    "#### Standard Tasks\n",
    "Many common datasets and benchmarks, e.g. GLUE, SuperGLUE, WMT, SQUAD, CNN/Daily Mail, etc. have been implemented as SeqIO Tasks/Mixtures and can be used directly.\n",
    "For the example run, you will fine-tune the model on the WMT14 English to German translation benchmark, which has been implemented as the `wmt_t2t_ende_v003` Task.\n",
    "\n",
    "#### Custom Tasks\n",
    "It is also possible to define your own custom task. See the SeqIO documentation for how to do this.  \n",
    "When defining a custom task, you have the option to cache it on disk before fine-tuning. Caching may improve performance for tasks with expensive pre-processing. By default, T5X expects tasks to be cached. To finetune on a task that has not been cached, set `--gin.USE_CACHED_TASKS=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write a Gin Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the pre-trained model and SeqIO Task/Mixture for your run, the next step is to configure your run using Gin. If you're not familiar with Gin, reading the T5X Gin Primer is recommended.\n",
    "\n",
    "T5X provides a Gin file that configures the T5X trainer for fine-tuning (located at `t5x/configs/runs/finetune.gin`), and expects a few params from you. These params can be specified in a separate Gin file, or via commandline flags. Following are the required params:\n",
    "\n",
    " - `INITIAL_CHECKPOINT_PATH`: This is the path to the pre-trained checkpoint (from Step 1). For the example run, set this to `gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000`.\n",
    " - `TRAIN_STEPS`: Number of fine-tuning steps. This includes the number of steps that the model was pre-trained for, so make sure to add the step number from the   `INITIAL_CHECKPOINT_PATH`. For the example run, to fine-tune for `20_000` steps, set this to `1_020_000`, since the initial checkpoint is the `1_000_000th` step.\n",
    " - `MIXTURE_OR_TASK_NAME`: This is the SeqIO Task or Mixture name to run (from Step 2). For the example run, set this to `wmt_t2t_ende_v003`.\n",
    " - `TASK_FEATURE_LENGTHS`: This is a dict mapping feature key to maximum int length for that feature. After preprocessing, features are truncated to the provided value. For the example run, set this to `{'inputs': 256, 'targets': 256}`.\n",
    " - `MODEL_DIR`: A path to write fine-tuned checkpoints to. In this case, a path to Google Cloud Storage.\n",
    " - `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be set to pretraining `batch_size * pretrained target_token_length`. For T5 and T5.1.1: `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.\n",
    "\n",
    " In addition to the above params, you will need to include `finetune.gin` and the Gin file for the pre-trained model, which for the example run is `t5_1_1/small.gin`.\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "```\n",
    "\n",
    "You will also need to import the Python module(s) that register SeqIO Tasks and Mixtures used in your run. For the example run, we add import `t5.data.tasks` since it is where `wmt_t2t_ende_v003` is registered.\n",
    "\n",
    "Finally, your Gin file should look like this:\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "\n",
    "# Register necessary SeqIO Tasks/Mixtures.\n",
    "import t5.data.tasks\n",
    "\n",
    "MIXTURE_OR_TASK_NAME = \"wmt_t2t_ende_v003\"\n",
    "TASK_FEATURE_LENGTHS = {\"inputs\": 256, \"targets\": 256}\n",
    "TRAIN_STEPS = 1_020_000  # 1000000 pre-trained steps + 20000 fine-tuning steps.\n",
    "DROPOUT_RATE = 0.0\n",
    "INITIAL_CHECKPOINT_PATH = \"gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000\"\n",
    "LOSS_NORMALIZING_FACTOR = 233472\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `t5x/examples/t5/t5_1_1/examples/small_wmt_finetune.gin` for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure Vertex AI Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'renatoleite-dev' # Change to your project id.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET = 'rl-language' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "VERSION = 'v01'\n",
    "MODEL_NAME = 'finetune-en-de'\n",
    "MODEL_DISPLAY_NAME = f'{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = 't5x-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET}/staging'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} --timeout=2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'cloud-tpu'\n",
    "ACCELERATOR_TYPE = 'TPU_V3'\n",
    "ACCELERATOR_NUM = 8\n",
    "REPLICA_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\n",
    "MODEL_DIR = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Data dir to save the processed dataset in \"gs://data_dir\" format.\n",
    "TFDS_DATA_DIR = f'gs://{BUCKET}/dataset/{MODEL_DISPLAY_NAME}'\n",
    "GIN_FILE = 'small_wmt_finetune.gin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": REPLICA_COUNT,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"/opt/conda/envs/t5x/bin/python\", \"/llm/t5x/t5x/train.py\"],\n",
    "            \"args\": [\n",
    "                f'--gin_file={GIN_FILE}',\n",
    "                f'--gin.MODEL_DIR=\"{MODEL_DIR}\"',\n",
    "                f'--tfds_data_dir=${TFDS_DATA_DIR}',\n",
    "                '--gin.USE_CACHED_TASKS=False'\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 't5x_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir =  os.path.join(WORKSPACE, job_name)\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "job.run(\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
