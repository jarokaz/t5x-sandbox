{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining a model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This page outlines the steps to pretrain a model with T5X on common tasks defined with [SeqIO](https://github.com/google/seqio/blob/main/README.md) on Vertex AI Training.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Pretraining a model with T5X on Vertex AI Training consists of the following steps:\n",
    "\n",
    "1.  Choose the model architecture.\n",
    "2.  Choose the SeqIO Task/Mixture to for training.\n",
    "3.  Write a Gin file that configures the model, SeqIO Task/Mixture and other\n",
    "    details of your pretraining run.\n",
    "4.  Configure and launch a training job on Vertex AI.\n",
    "5.  Monitor your job.\n",
    "\n",
    "These steps are explained in detail in the following sections. An example run that trains a T5 1.1 Small checkpoint from scratch on the C4 dataset using the span corruption pretraining objective is also showcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose a model architecture\n",
    "\n",
    "To train a model, you need a Gin config file that defines the model params. For\n",
    "your convenience, Gin configs for common models have been made available for use\n",
    "in T5X. Following is a list of these models and their Gin locations.\n",
    "\n",
    "Model                                 | Gin File Location\n",
    "------------------------------------- | -----------------\n",
    "T5 Small                              | [t5_1_0/small.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_0/small.gin)\n",
    "T5 Base                               | [t5_1_0/base.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_0/base.gin)\n",
    "T5 Large                              | [t5_1_0/large.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_0/large.gin)\n",
    "T5 3B                                 | [t5_1_0/3B.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_0/3B.gin)\n",
    "T5 11B                                | [t5_1_0/11B.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_0/11B.gin)\n",
    "T5 1.1 Small                          | [t5_1_1/small.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/small.gin)\n",
    "T5 1.1 Base                           | [t5_1_1/base.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/base.gin)\n",
    "T5 1.1 Large                          | [t5_1_1/large.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/large.gin)\n",
    "T5 1.1 XL                             | [t5_1_1/xl.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/xl.gin)\n",
    "T5 1.1 XXL                            | [t5_1_1/xxl.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/xxl.gin)\n",
    "MT5 Small                             | [mt5/small.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/mt5/small.gin)\n",
    "MT5 Base                              | [mt5/base.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/mt5/base.gin)\n",
    "MT5 Large                             | [mt5/large.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/mt5/large.gin)\n",
    "MT5 XL                                | [mt5/xl.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/mt5/xl.gin)\n",
    "MT5 XXL                               | [mt5/xxl.gin](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/mt5/xxl.gin)\n",
    "\n",
    "For the example run, you will use the T5 1.1 Small model. The Gin file for this\n",
    "model is located at\n",
    "[`/t5x/examples/t5/t5_1_1/1_1_small.gin`](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/small.gin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose a SeqIO Task/Mixture\n",
    "\n",
    "A SeqIO Task encapsulates the data source, the preprocessing logic to be\n",
    "performed on the data before querying the model, the postprocessing logic to be\n",
    "performed on model outputs, and the metrics to be computed given the\n",
    "postprocessed outputs and targets. A SeqIO Mixture denotes a collection of Tasks\n",
    "and enables pretraining a model on multiple Tasks simultaneously.\n",
    "\n",
    "Many common datasets and benchmarks, e.g. [GLUE](https://gluebenchmark.com/),\n",
    "[SuperGLUE](https://super.gluebenchmark.com/),\n",
    "[WMT](https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate),\n",
    "[SQUAD](https://rajpurkar.github.io/SQuAD-explorer/),\n",
    "[CNN/Daily Mail](https://github.com/abisee/cnn-dailymail), etc. have been\n",
    "implemented as SeqIO Tasks/Mixtures and can be used directly. These\n",
    "Tasks/Mixtures are defined in\n",
    "[`third_party/py/t5/data/tasks.py`](https://github.com/google-research/text-to-text-transfer-transformer/tree/main/t5/data/tasks.py)\n",
    "and\n",
    "[`third_party/py/t5/data/mixtures.py`](https://github.com/google-research/text-to-text-transfer-transformer/tree/main/t5/data/mixtures.py).\n",
    "\n",
    "For the example run, you will train the model on\n",
    "[`c4_v220_span_corruption`](https://github.com/google-research/text-to-text-transfer-transformer/tree/main/t5/data/tasks.py?l=42&rcl=370153959)\n",
    "Task that implements the span corruption pretraining objective using the C4\n",
    "dataset. This is the final pretraining Task used in the\n",
    "[T5 paper](https://arxiv.org/pdf/1910.10683.pdf%C3%82%C2%A0).\n",
    "\n",
    "TIP: Want to use a custom Task or Mixture? See section below called \"Adding\n",
    "SeqIO Task/Mixture modules and Gin files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write a Gin Config\n",
    "\n",
    "After choosing the model architecture and SeqIO Task/Mixture for your run, the\n",
    "next step is to configure your run using Gin. If you're not familiar with Gin,\n",
    "reading the [T5X Gin Primer](gin.md) is recommended.\n",
    "\n",
    "T5X provides a Gin file that configures the T5X trainer for pretraining (located\n",
    "at [`runs/pretrain.gin`](https://github.com/google-research/t5x/tree/main/t5x/configs/runs/pretrain.gin)),\n",
    "and expects a few params from you. These params can be specified in a separate\n",
    "Gin file, or via commandline flags. Following are the required params:\n",
    "\n",
    "+   `TRAIN_STEPS`: Number of training steps. For the example run, set this to\n",
    "    `100_000`.\n",
    "+   `MIXTURE_OR_TASK_NAME`: This is the SeqIO Task or Mixture name to run (from\n",
    "    Step 2). For the example run, set this to `'c4_v220_span_corruption'`.\n",
    "+   `TASK_FEATURE_LENGTHS`: This is a dict mapping feature key to maximum int\n",
    "    length for that feature. After preprocessing, features are truncated to the\n",
    "    provided value. For the example run, set this to `{\"inputs\": 512, \"targets\":\n",
    "    114}`, following the original T5 pretraining setup.\n",
    "+   `MODEL_DIR`: A path to write pretrained checkpoints to.\n",
    "\n",
    "In addition to the above params, you will need to import\n",
    "[`pretrain.gin`](https://github.com/google-research/t5x/tree/main/t5x/configs/runs/pretrain.gin)\n",
    "and the Gin file for the pretrained model, which for the example run is\n",
    "[`t5_1_1/small.gin`](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/small.gin).\n",
    "\n",
    "```gin\n",
    "include 't5x/configs/runs/pretrain.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "```\n",
    "\n",
    "Note that the `include` statements can use relative paths in this example for\n",
    "which you will pass an appropriate `gin_search_paths` flag to locate these files\n",
    "when launching your run. However, we recommend that you use absolute paths.\n",
    "\n",
    "You will also need to import the Python module(s) that register SeqIO Tasks and\n",
    "Mixtures used in your run. For the example run, we add `import t5.data.mixtures`\n",
    "since it is where 'glue_v002_proportional' is registered. Note that this module\n",
    "must also be included as a dependency in the T5X trainer\n",
    "[binary](https://github.com/google-research/t5x/tree/main/t5x/BUILD;l=74;rcl=398627055). Most\n",
    "common Task/Mixture modules, such as this one, are already included. If your\n",
    "module is not included, see the [Advanced Topics section](#custom-t5x-binaries)\n",
    "at the end of this tutorial for instructions to add it.\n",
    "\n",
    "Finally, your Gin file should look like this:\n",
    "\n",
    "```gin\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "include 't5x/configs/runs/pretrain.gin'\n",
    "\n",
    "# Register necessary SeqIO Tasks/Mixtures.\n",
    "import t5.data.mixtures\n",
    "\n",
    "MIXTURE_OR_TASK_NAME = \"c4_v220_span_corruption\"\n",
    "TASK_FEATURE_LENGTHS = {\"inputs\": 512, \"targets\": 114}\n",
    "TRAIN_STEPS = 10000\n",
    "DROPOUT_RATE = 0.0\n",
    "BATCH_SIZE = 256\n",
    "```\n",
    "\n",
    "See\n",
    "[`t5x/examples/t5/t5_1_1/examples/small_c4_pretrain.gin`](https://github.com/google-research/t5x/tree/main/t5x/examples/t5/t5_1_1/examples/small_c4_pretrain.gin)\n",
    "for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure and launch a Vertex AI Training job to fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'renatoleite-dev' # Change to your project id.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET = 'rl-language' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "VERSION = 'v01'\n",
    "MODEL_NAME = 'pretrain-en-de'\n",
    "MODEL_DISPLAY_NAME = f'{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = 't5x-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET}/staging'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} --timeout=2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'cloud-tpu'\n",
    "ACCELERATOR_TYPE = 'TPU_V3'\n",
    "ACCELERATOR_NUM = 8\n",
    "REPLICA_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\n",
    "MODEL_DIR = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Data dir to save the processed dataset in \"gs://data_dir\" format.\n",
    "TFDS_DATA_DIR = f'gs://{BUCKET}/dataset/{MODEL_DISPLAY_NAME}'\n",
    "GIN_FILE = './base_wmt_from_scratch.gin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": REPLICA_COUNT,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"/opt/conda/envs/t5x/bin/python\", \"/llm/t5x/t5x/train.py\"],\n",
    "            \"args\": [\n",
    "                f'--gin_file={GIN_FILE}',\n",
    "                f'--gin.MODEL_DIR=\"{MODEL_DIR}\"',\n",
    "                f'--tfds_data_dir={TFDS_DATA_DIR}',\n",
    "                '--gin.USE_CACHED_TASKS=False'\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 't5x_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir =  os.path.join(WORKSPACE, job_name)\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "job.run(\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('t5x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e08c341cd3069ddbdf52929b0982b0e2df3b90883897a503b8f5cfb9ad9e4016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
