{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning SQuAD with T5x on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page outlines the steps to fine-tune an existing pre-trained model with T5X on common downstream tasks defined with SeqIO using Vertex AI Training. This is one of the simplest and most common use cases of T5X. If you're new to T5X, this tutorial is the recommended starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a model with T5X consists of the following steps:\n",
    "\n",
    "1. Choose the pre-trained model to fine-tune.\n",
    "2. Choose the SeqIO Task/Mixture to fine-tune the model on.\n",
    "3. Write a Gin file that configures the pre-trained model, SeqIO Task/Mixture and other details of your fine-tuning run.\n",
    "4. Configure a Vertex AI Training job to fine tune the model.\n",
    "5. Monitor your job and parse metrics.\n",
    "\n",
    "These steps are explained in detail in the following sections. An example run that fine-tunes a T5-small checkpoint on WMT14 English to German translation benchmark is also showcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Choose a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pre-trained model, you need a Gin config file that defines the model params, and the model checkpoint to load from. For your convenience, TensorFlow checkpoints and Gin configs for common T5 pre-trained models have been made available for use in T5X. A list of all the available pre-trained models (with model checkpoints and Gin config files) are available in the Models documentation.\n",
    "\n",
    "For the example run, you will use the T5 1.1 Small model. The Gin file for this model is located at `/t5x/examples/t5/t5_1_1/small.gin`, and the checkpoint is located at `gs://t5-data/pretrained_models/t5x/t5_1_1_small`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose a SeqIO Task/Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SeqIO Task encapsulates the data source, the preprocessing logic to be performed on the data before querying the model, the postprocessing logic to be performed on model outputs, and the metrics to be computed given the postprocessed outputs and targets. A SeqIO Mixture denotes a collection of Tasks and enables fine-tuning a model on multiple Tasks simultaneously.\n",
    "\n",
    "#### Standard Tasks\n",
    "Many common datasets and benchmarks, e.g. GLUE, SuperGLUE, WMT, SQUAD, CNN/Daily Mail, etc. have been implemented as SeqIO Tasks/Mixtures and can be used directly.\n",
    "For the example run, you will fine-tune the model on the SQuAD Q&A benchmark, which has been implemented as the `squad_v010_allanswers` Task.\n",
    "\n",
    "The details of the implementation can be found here:\n",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/7db665af4fe395398a0fc20038632584cca2a99a/t5/data/tasks.py#L336\n",
    "\n",
    "#### Custom Tasks\n",
    "It is also possible to define your own custom task. See the SeqIO documentation for how to do this.  \n",
    "When defining a custom task, you have the option to cache it on disk before fine-tuning. Caching may improve performance for tasks with expensive pre-processing. By default, T5X expects tasks to be cached. To finetune on a task that has not been cached, set `--gin.USE_CACHED_TASKS=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write a Gin Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the pre-trained model and SeqIO Task/Mixture for your run, the next step is to configure your run using Gin. If you're not familiar with Gin, reading the T5X Gin Primer is recommended.\n",
    "\n",
    "T5X provides a Gin file that configures the T5X trainer for fine-tuning (located at `t5x/configs/runs/finetune.gin`), and expects a few params from you. These params can be specified in a separate Gin file, or via commandline flags. Following are the required params:\n",
    "\n",
    " - `INITIAL_CHECKPOINT_PATH`: This is the path to the pre-trained checkpoint (from Step 1). For the example run, set this to `gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000`.\n",
    " - `TRAIN_STEPS`: Number of fine-tuning steps. This includes the number of steps that the model was pre-trained for, so make sure to add the step number from the   `INITIAL_CHECKPOINT_PATH`. For the example run, to fine-tune for `20_000` steps, set this to `1_020_000`, since the initial checkpoint is the `1_000_000th` step.\n",
    " - `MIXTURE_OR_TASK_NAME`: This is the SeqIO Task or Mixture name to run (from Step 2). For the example run, set this to `squad_v010_allanswers`.\n",
    " - `TASK_FEATURE_LENGTHS`: This is a dict mapping feature key to maximum int length for that feature. After preprocessing, features are truncated to the provided value. For the example run, set this to `{'inputs': 256, 'targets': 256}`.\n",
    " - `MODEL_DIR`: A path to write fine-tuned checkpoints to. In this case, a path to Google Cloud Storage.\n",
    " - `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be set to pretraining `batch_size * pretrained target_token_length`. For T5 and T5.1.1: `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.\n",
    "\n",
    " In addition to the above params, you will need to include `finetune.gin` and the Gin file for the pre-trained model, which for the example run is `t5_1_1/small.gin`.\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "```\n",
    "\n",
    "You will also need to import the Python module(s) that register SeqIO Tasks and Mixtures used in your run. For the example run, we add import `t5.data.tasks` since it is where `squad_v010_allanswers` is registered.\n",
    "\n",
    "Finally, your Gin file should look like this:\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "\n",
    "# Register necessary SeqIO Tasks/Mixtures.\n",
    "import t5.data.tasks\n",
    "\n",
    "MIXTURE_OR_TASK_NAME = \"squad_v010_allanswers\"\n",
    "TASK_FEATURE_LENGTHS = {\"inputs\": 256, \"targets\": 256}\n",
    "TRAIN_STEPS = 1_020_000  # 1000000 pre-trained steps + 20000 fine-tuning steps.\n",
    "DROPOUT_RATE = 0.0\n",
    "INITIAL_CHECKPOINT_PATH = \"gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000\"\n",
    "LOSS_NORMALIZING_FACTOR = 233472\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `t5x/examples/t5/t5_1_1/examples/small_wmt_finetune.gin` for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure and launch a Vertex AI Training job to fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define variables for training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'renatoleite-dev' # Change to your project id.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET = 'rl-language' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "VERSION = 'v01'\n",
    "MODEL_NAME = 'finetune-squad'\n",
    "MODEL_DISPLAY_NAME = f'{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = 't5x-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Vertex AI client and log metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "EXPERIMENT_ID = f'{MODEL_DISPLAY_NAME}-{TIMESTAMP}'\n",
    "EXECUTION_NAME = f'execution-1'\n",
    "RUN_NAME = 'run-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET}/staging',\n",
    "    experiment=EXPERIMENT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/375468928805/locations/us-central1/metadataStores/default/contexts/finetune-squad-v01-202207131408-run-1 to Experiment: finetune-squad-v01-202207131408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.metadata.experiment_run_resource.ExperimentRun at 0x7f2f2b82c5b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_ai.start_run(RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaparams = {}\n",
    "metaparams['emb_dim'] = 512\n",
    "metaparams['num_heads'] = 6\n",
    "metaparams['num_encoder_layers'] = 8\n",
    "metaparams['num_decoder_layers'] = 8\n",
    "metaparams['head_dim'] = 64\n",
    "metaparams['mlp_dim'] = 1024\n",
    "metaparams['inputs_feature_len'] = 256\n",
    "metaparams['outputs_feature_len'] = 256\n",
    "metaparams['mixture_task_name'] = 'squad_v010_allanswers'\n",
    "vertex_ai.log_params(metaparams)\n",
    "\n",
    "hyperparams = {}\n",
    "hyperparams['train_steps'] = 1_020_000\n",
    "hyperparams['dropout_rate'] = 0.0\n",
    "hyperparams['loss_normalizing_factor'] = 233472\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seqio_artifact = vertex_ai.Artifact.create(\n",
    "    schema_title=\"system.Dataset\", display_name='seqio_task_mixture', uri='squad_v010_allanswers'\n",
    ")\n",
    "\n",
    "model_artifact = vertex_ai.Artifact.create(\n",
    "    schema_title=\"system.Model\", display_name='squad_finetuned_model', uri=WORKSPACE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=EXECUTION_NAME\n",
    ") as execution:\n",
    "    execution.assign_input_artifacts([dataset_seqio_artifact])\n",
    "    execution.assign_output_artifacts([model_artifact])\n",
    "\n",
    "    vertex_ai.log_metrics(\n",
    "        {\"lineage\": execution.get_output_artifacts()[0].lineage_console_uri}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build docker image to run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} --timeout=2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define infra and submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'cloud-tpu'\n",
    "ACCELERATOR_TYPE = 'TPU_V3'\n",
    "ACCELERATOR_NUM = 8\n",
    "REPLICA_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\n",
    "MODEL_DIR = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Data dir to save the processed dataset in \"gs://data_dir\" format.\n",
    "TFDS_DATA_DIR = f'gs://{BUCKET}/dataset/{MODEL_DISPLAY_NAME}'\n",
    "GIN_FILE = './small_finetune_squad.gin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": REPLICA_COUNT,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"/opt/conda/envs/t5x/bin/python\", \"/llm/t5x/t5x/train.py\"],\n",
    "            \"args\": [\n",
    "                f'--gin_file={GIN_FILE}',\n",
    "                f'--gin.MODEL_DIR=\"{MODEL_DIR}\"',\n",
    "                f'--tfds_data_dir={TFDS_DATA_DIR}',\n",
    "                '--gin.USE_CACHED_TASKS=False'\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 't5x_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir =  os.path.join(WORKSPACE, job_name)\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Explore and log metrics\n",
    "\n",
    "After fine-tuning has completed, you can parse metrics into CSV format using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_VAL_DIR=f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}/inference_eval/*'\n",
    "VAL_DIR = './inference_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir {VAL_DIR}\n",
    "! gsutil -m cp -r {GCS_VAL_DIR} {VAL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m t5.scripts.parse_tb \\\n",
    "  --summary_dir={VAL_DIR} \\\n",
    "  --seqio_summaries \\\n",
    "  --out_file=./results.csv \\\n",
    "  --alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>SQuAD (EM)</th>\n",
       "      <th>SQuAD (F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001000</td>\n",
       "      <td>17.899715</td>\n",
       "      <td>32.277443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002000</td>\n",
       "      <td>67.445600</td>\n",
       "      <td>76.894325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003000</td>\n",
       "      <td>69.943240</td>\n",
       "      <td>79.272090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004000</td>\n",
       "      <td>71.021760</td>\n",
       "      <td>81.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005000</td>\n",
       "      <td>71.892150</td>\n",
       "      <td>81.643790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006000</td>\n",
       "      <td>71.589400</td>\n",
       "      <td>81.672770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007000</td>\n",
       "      <td>72.024600</td>\n",
       "      <td>81.407530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008000</td>\n",
       "      <td>71.882690</td>\n",
       "      <td>81.754150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009000</td>\n",
       "      <td>71.343420</td>\n",
       "      <td>81.473366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010000</td>\n",
       "      <td>70.104065</td>\n",
       "      <td>80.828880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1011000</td>\n",
       "      <td>68.968780</td>\n",
       "      <td>80.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1012000</td>\n",
       "      <td>69.186380</td>\n",
       "      <td>80.236090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1013000</td>\n",
       "      <td>69.337746</td>\n",
       "      <td>80.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1014000</td>\n",
       "      <td>68.372750</td>\n",
       "      <td>79.655820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1015000</td>\n",
       "      <td>68.893100</td>\n",
       "      <td>79.904680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1016000</td>\n",
       "      <td>67.511826</td>\n",
       "      <td>79.060680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1017000</td>\n",
       "      <td>68.410600</td>\n",
       "      <td>79.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1018000</td>\n",
       "      <td>67.549670</td>\n",
       "      <td>79.046140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1019000</td>\n",
       "      <td>67.805110</td>\n",
       "      <td>78.927345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1020000</td>\n",
       "      <td>67.436140</td>\n",
       "      <td>78.732790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       step  SQuAD (EM)  SQuAD (F1)\n",
       "0   1001000   17.899715   32.277443\n",
       "1   1002000   67.445600   76.894325\n",
       "2   1003000   69.943240   79.272090\n",
       "3   1004000   71.021760   81.044600\n",
       "4   1005000   71.892150   81.643790\n",
       "5   1006000   71.589400   81.672770\n",
       "6   1007000   72.024600   81.407530\n",
       "7   1008000   71.882690   81.754150\n",
       "8   1009000   71.343420   81.473366\n",
       "9   1010000   70.104065   80.828880\n",
       "10  1011000   68.968780   80.334500\n",
       "11  1012000   69.186380   80.236090\n",
       "12  1013000   69.337746   80.107700\n",
       "13  1014000   68.372750   79.655820\n",
       "14  1015000   68.893100   79.904680\n",
       "15  1016000   67.511826   79.060680\n",
       "16  1017000   68.410600   79.299400\n",
       "17  1018000   67.549670   79.046140\n",
       "18  1019000   67.805110   78.927345\n",
       "19  1020000   67.436140   78.732790"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>SQuAD (EM)</th>\n",
       "      <th>SQuAD (F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>step</td>\n",
       "      <td>1008000.0</td>\n",
       "      <td>1008000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    step  SQuAD (EM)  SQuAD (F1)\n",
       "21  step   1008000.0   1008000.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008000.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['SQuAD (EM)'][-1:].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['max_squad_em'] = results[-2:-1]['SQuAD (EM)'].values[0]\n",
    "metrics['max_squad_f1'] = results[-2:-1]['SQuAD (F1)'].values[0]\n",
    "metrics['step_squad_em'] = results[-1:]['SQuAD (EM)'].values[0]\n",
    "metrics['step_squad_f1'] = results[-1:]['SQuAD (F1)'].values[0]\n",
    "vertex_ai.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Explanations\n",
    "\n",
    "By default, t5x logs many metrics to TensorBoard, many of these seem similar but\n",
    "have important distinctions.\n",
    "\n",
    "The first two graphs you will see are the `accuracy` and `cross_ent_loss`\n",
    "graphs. These are the *token-level teacher-forced* accuracy and cross entropy\n",
    "loss respectively. Each of these graphs can have multiple curves on them. The\n",
    "first curve is the `train` curve. This is calculated as a running sum than is\n",
    "then normalized over the whole training set. The second class of curves have the\n",
    "form `training_eval/${task_name}`. These curves are created by running a subset\n",
    "(controlled by the `eval_steps` parameter of the main train function) of the\n",
    "validation split of `${task_name}` through the model and calculating these\n",
    "metrics using teacher-forcing. These graphs can commonly be used to find\n",
    "\"failure to learn\" cases and as a warning sign of overfitting, but these are\n",
    "often not the final metrics one would report on.\n",
    "\n",
    "The second set of graphs are the ones under the collapsible `eval` section in\n",
    "TensorBoard. These graphs are created based on the `metric_fns` defined in the\n",
    "SeqIO task. The curves on these graphs have the form\n",
    "`inference_eval/${task_name}`. Values are calculated by running the whole\n",
    "validation split through the model in inference mode, commonly auto-regressive\n",
    "decoding or output scoring. Most likely these are the metrics that will be\n",
    "reported.\n",
    "\n",
    "More information about the configuration of the datasets used for these\n",
    "different metrics can be found [here](#train-train-eval-and-infer-eval).\n",
    "\n",
    "In summary, the metric you actually care about most likely lives under the\n",
    "`eval` tab rather, than in the `accuracy` graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('t5x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e08c341cd3069ddbdf52929b0982b0e2df3b90883897a503b8f5cfb9ad9e4016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
