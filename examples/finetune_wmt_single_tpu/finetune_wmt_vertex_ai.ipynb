{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning a Translation Model with T5X on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page outlines the steps to fine-tune an existing pre-trained model with T5X on common downstream tasks defined with SeqIO using Vertex AI Training. This is one of the simplest and most common use cases of T5X. If you're new to T5X, this tutorial is the recommended starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a model with T5X consists of the following steps:\n",
    "\n",
    "1. Choose the pre-trained model to fine-tune.\n",
    "2. Choose the SeqIO Task/Mixture to fine-tune the model on.\n",
    "3. Write a Gin file that configures the pre-trained model, SeqIO Task/Mixture and other details of your fine-tuning run.\n",
    "4. Configure a Vertex AI Training job to fine tune the model.\n",
    "5. Monitor your job and parse metrics.\n",
    "\n",
    "These steps are explained in detail in the following sections. An example run that fine-tunes a T5-small checkpoint on WMT14 English to German translation benchmark is also showcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Choose a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pre-trained model, you need a Gin config file that defines the model params, and the model checkpoint to load from. For your convenience, TensorFlow checkpoints and Gin configs for common T5 pre-trained models have been made available for use in T5X. A list of all the available pre-trained models (with model checkpoints and Gin config files) are available in the Models documentation.\n",
    "\n",
    "For the example run, you will use the T5 1.1 Small model. The Gin file for this model is located at `/t5x/examples/t5/t5_1_1/small.gin`, and the checkpoint is located at `gs://t5-data/pretrained_models/t5x/t5_1_1_small`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose a SeqIO Task/Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SeqIO Task encapsulates the data source, the preprocessing logic to be performed on the data before querying the model, the postprocessing logic to be performed on model outputs, and the metrics to be computed given the postprocessed outputs and targets. A SeqIO Mixture denotes a collection of Tasks and enables fine-tuning a model on multiple Tasks simultaneously.\n",
    "\n",
    "#### Standard Tasks\n",
    "Many common datasets and benchmarks, e.g. GLUE, SuperGLUE, WMT, SQUAD, CNN/Daily Mail, etc. have been implemented as SeqIO Tasks/Mixtures and can be used directly.\n",
    "For the example run, you will fine-tune the model on the WMT14 English to German translation benchmark, which has been implemented as the `wmt_t2t_ende_v003` Task.\n",
    "\n",
    "#### Custom Tasks\n",
    "It is also possible to define your own custom task. See the SeqIO documentation for how to do this.  \n",
    "When defining a custom task, you have the option to cache it on disk before fine-tuning. Caching may improve performance for tasks with expensive pre-processing. By default, T5X expects tasks to be cached. To finetune on a task that has not been cached, set `--gin.USE_CACHED_TASKS=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write a Gin Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the pre-trained model and SeqIO Task/Mixture for your run, the next step is to configure your run using Gin. If you're not familiar with Gin, reading the T5X Gin Primer is recommended.\n",
    "\n",
    "T5X provides a Gin file that configures the T5X trainer for fine-tuning (located at `t5x/configs/runs/finetune.gin`), and expects a few params from you. These params can be specified in a separate Gin file, or via commandline flags. Following are the required params:\n",
    "\n",
    " - `INITIAL_CHECKPOINT_PATH`: This is the path to the pre-trained checkpoint (from Step 1). For the example run, set this to `gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000`.\n",
    " - `TRAIN_STEPS`: Number of fine-tuning steps. This includes the number of steps that the model was pre-trained for, so make sure to add the step number from the   `INITIAL_CHECKPOINT_PATH`. For the example run, to fine-tune for `20_000` steps, set this to `1_020_000`, since the initial checkpoint is the `1_000_000th` step.\n",
    " - `MIXTURE_OR_TASK_NAME`: This is the SeqIO Task or Mixture name to run (from Step 2). For the example run, set this to `wmt_t2t_ende_v003`.\n",
    " - `TASK_FEATURE_LENGTHS`: This is a dict mapping feature key to maximum int length for that feature. After preprocessing, features are truncated to the provided value. For the example run, set this to `{'inputs': 256, 'targets': 256}`.\n",
    " - `MODEL_DIR`: A path to write fine-tuned checkpoints to. In this case, a path to Google Cloud Storage.\n",
    " - `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be set to pretraining `batch_size * pretrained target_token_length`. For T5 and T5.1.1: `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.\n",
    "\n",
    " In addition to the above params, you will need to include `finetune.gin` and the Gin file for the pre-trained model, which for the example run is `t5_1_1/small.gin`.\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "```\n",
    "\n",
    "You will also need to import the Python module(s) that register SeqIO Tasks and Mixtures used in your run. For the example run, we add import `t5.data.tasks` since it is where `wmt_t2t_ende_v003` is registered.\n",
    "\n",
    "Finally, your Gin file should look like this:\n",
    "\n",
    "```\n",
    "include 't5x/configs/runs/finetune.gin'\n",
    "include 't5x/examples/t5/t5_1_1/small.gin'\n",
    "\n",
    "# Register necessary SeqIO Tasks/Mixtures.\n",
    "import t5.data.tasks\n",
    "\n",
    "MIXTURE_OR_TASK_NAME = \"wmt_t2t_ende_v003\"\n",
    "TASK_FEATURE_LENGTHS = {\"inputs\": 256, \"targets\": 256}\n",
    "TRAIN_STEPS = 1_020_000  # 1000000 pre-trained steps + 20000 fine-tuning steps.\n",
    "DROPOUT_RATE = 0.0\n",
    "INITIAL_CHECKPOINT_PATH = \"gs://t5-data/pretrained_models/t5x/t5_1_1_small/checkpoint_1000000\"\n",
    "LOSS_NORMALIZING_FACTOR = 233472\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `t5x/examples/t5/t5_1_1/examples/small_wmt_finetune.gin` for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure and launch a Vertex AI Training job to fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'renatoleite-dev' # Change to your project id.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET = 'rl-language' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "VERSION = 'v01'\n",
    "MODEL_NAME = 'finetune-en-de'\n",
    "MODEL_DISPLAY_NAME = f'{MODEL_NAME}-{VERSION}'\n",
    "\n",
    "# Staging bucket for Vertex AI\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions for training\n",
    "IMAGE_NAME = 't5x-base'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "# Model dir to save logs, ckpts, etc.\n",
    "MODEL_DIR = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}/{TIMESTAMP}'\n",
    "\n",
    "# Data dir to save the processed dataset\n",
    "TFDS_DATA_DIR = f'gs://{BUCKET}/dataset/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Gin file and run mode\n",
    "GIN_FILE = 'small_finetune_wmt.gin'\n",
    "GIN_FILE_GCS = f'gs://{BUCKET}/staging/{GIN_FILE}'\n",
    "RUN_MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy gin file to GCS bucket and use the local GCSFuse mount\n",
    "! gsutil cp {GIN_FILE} {GIN_FILE_GCS}\n",
    "GIN_FILE_GCS = GIN_FILE_GCS.replace('gs://', '/gcs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Vertex AI client and log metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = f'{MODEL_DISPLAY_NAME}-{TIMESTAMP}'\n",
    "EXECUTION_NAME = f'execution-1'\n",
    "RUN_NAME = 'run-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET}/staging',\n",
    "    experiment=EXPERIMENT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.start_run(RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=EXECUTION_NAME\n",
    ") as execution:\n",
    "\n",
    "    dataset_seqio_artifact = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Dataset\", display_name='tfds_dataset', uri=TFDS_DATA_DIR\n",
    "    )\n",
    "\n",
    "    with open(GIN_FILE) as fp:\n",
    "        gin_content = fp.read()\n",
    "\n",
    "    gin_config_artifact = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Artifact\", \n",
    "        display_name='gin_configuration_file', \n",
    "        uri=GIN_FILE_GCS.replace('/gcs/', 'gs://'),\n",
    "        metadata= {\n",
    "            'gin_file': gin_content\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_artifact = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Model\", display_name='wmt_finetuned_model', uri=MODEL_DIR\n",
    "    )\n",
    "\n",
    "    execution.assign_input_artifacts([dataset_seqio_artifact, gin_config_artifact])\n",
    "    execution.assign_output_artifacts([model_artifact])\n",
    "\n",
    "    vertex_ai.log_metrics(\n",
    "        {\"lineage\": execution.get_output_artifacts()[0].lineage_console_uri}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define infra and submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'cloud-tpu'\n",
    "ACCELERATOR_TYPE = 'TPU_V3'\n",
    "ACCELERATOR_NUM = 8\n",
    "REPLICA_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": REPLICA_COUNT,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"args\": [\n",
    "                f'--run_mode={RUN_MODE}',\n",
    "                f'--gin_file={GIN_FILE_GCS}',\n",
    "                f'--gin.MODEL_DIR=\"{MODEL_DIR}\"',\n",
    "                f'--tfds_data_dir={TFDS_DATA_DIR}',\n",
    "                '--gin.USE_CACHED_TASKS=False'\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 't5x_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir =  os.path.join(WORKSPACE, job_name)\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "job.run(\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Explore metrics\n",
    "\n",
    "After fine-tuning has completed, you can parse metrics into CSV format using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_VAL_DIR=f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}/inference_eval/*'\n",
    "VAL_DIR = './inference_eval'\n",
    "OUTPUT_FILE = './results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir {VAL_DIR}\n",
    "! gsutil -m cp -r {GCS_VAL_DIR} {VAL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m t5.scripts.parse_tb \\\n",
    "  --summary_dir={VAL_DIR} \\\n",
    "  --seqio_summaries \\\n",
    "  --out_file={OUTPUT_FILE} \\\n",
    "  --alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results.csv', sep=',')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['max_squad_em'] = results[-2:-1]['SQuAD (EM)'].values[0]\n",
    "metrics['max_squad_f1'] = results[-2:-1]['SQuAD (F1)'].values[0]\n",
    "metrics['step_squad_em'] = results[-1:]['SQuAD (EM)'].values[0]\n",
    "metrics['step_squad_f1'] = results[-1:]['SQuAD (F1)'].values[0]\n",
    "vertex_ai.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse with Vertex AI Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_INSTANCE_NAME = 't5x-analyse'\n",
    "VALIDATION_TB_LOGS = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}/inference_eval'\n",
    "TRAINING_TB_LOGS = f'gs://{BUCKET}/model/{MODEL_DISPLAY_NAME}/training_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai tensorboards create --display-name={TENSORBOARD_INSTANCE_NAME} --region={REGION} --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_id = ! gcloud ai tensorboards list --filter=\"displayName=t5x-analyse\" --format=\"value(name)\" --region=us-central1 --limit=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tb-gcp-uploader --tensorboard_resource_name \\\n",
    "  {tensorboard_id[1]} \\\n",
    "  --logdir={VALIDATION_TB_LOGS} \\\n",
    "  --experiment_name={EXPERIMENT_ID} --one_shot=True\n",
    "\n",
    "! tb-gcp-uploader --tensorboard_resource_name \\\n",
    "  {tensorboard_id[1]} \\\n",
    "  --logdir={TRAINING_TB_LOGS} \\\n",
    "  --experiment_name={EXPERIMENT_ID} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the URL presented in the output of this command and analyse the training and inference logs.\n",
    "\n",
    "![Tensorboard](./images/tb-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Explanations\n",
    "\n",
    "By default, t5x logs many metrics to TensorBoard, many of these seem similar but\n",
    "have important distinctions.\n",
    "\n",
    "The first two graphs you will see are the `accuracy` and `cross_ent_loss`\n",
    "graphs. These are the *token-level teacher-forced* accuracy and cross entropy\n",
    "loss respectively. Each of these graphs can have multiple curves on them. The\n",
    "first curve is the `train` curve. This is calculated as a running sum than is\n",
    "then normalized over the whole training set. The second class of curves have the\n",
    "form `training_eval/${task_name}`. These curves are created by running a subset\n",
    "(controlled by the `eval_steps` parameter of the main train function) of the\n",
    "validation split of `${task_name}` through the model and calculating these\n",
    "metrics using teacher-forcing. These graphs can commonly be used to find\n",
    "\"failure to learn\" cases and as a warning sign of overfitting, but these are\n",
    "often not the final metrics one would report on.\n",
    "\n",
    "The second set of graphs are the ones under the collapsible `eval` section in\n",
    "TensorBoard. These graphs are created based on the `metric_fns` defined in the\n",
    "SeqIO task. The curves on these graphs have the form\n",
    "`inference_eval/${task_name}`. Values are calculated by running the whole\n",
    "validation split through the model in inference mode, commonly auto-regressive\n",
    "decoding or output scoring. Most likely these are the metrics that will be\n",
    "reported.\n",
    "\n",
    "More information about the configuration of the datasets used for these\n",
    "different metrics can be found [here](#train-train-eval-and-infer-eval).\n",
    "\n",
    "In summary, the metric you actually care about most likely lives under the\n",
    "`eval` tab rather, than in the `accuracy` graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
